{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Internal Document\n",
    "# DAND Project WeRateDogs: Wrangling report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WeRateDogs is a Twitter account that rates people's dogs with a humorous comment about the dog. We wrangle the WeRateDogs Tweets. We provide a cleaned dataset for further analysis.\n",
    "\n",
    "We combine three datasets:\n",
    "1. WeRateDogs Twitter archive download by WeRateDogs.\n",
    "2. Data downloaded using the Twitter API.\n",
    "3. Predictions of the pictures' content.\n",
    "\n",
    "Concerning 1: WeRateDogs downloaded their Twitter archive and sent it to Udacity. The format is CSV. Some enhancement of the data is included. Namely, from the Tweet texts, the dogs' names and dogs stages are extracted. Dogs stages are made up of dog categories from WeRateDogs.\n",
    "\n",
    "Concerning 2: Using the Twitter API all information from WeRateDogs can be downloaded. The exceptions are the enhancements mentioned under (1) and the predictions of the pictures.\n",
    "\n",
    "Concerning 3: The images in the WeRateDogs Twitter archive were run through a neural network to classify breeds of dogs. The data is provided as a link to TSV file. \n",
    "\n",
    "Much Tweet information can be obtained through both (1) and (2). Because the ETL steps obtaining the data directly from the source are more transparent, we prefer to collect the data from the Twitter API. The API is called Tweepy.\n",
    "\n",
    "Our approach is to collect all the data which is available via the API. We select the relevant columns later. Thus we can easily expand the analysis to data not considered before.\n",
    "\n",
    "Our goal is to produce a high-quality dataset. We prefer a complete dataset over collecting as much data as possible. This means we combine the information from all sources and drop observations with missing data. Most notably, the predictions in (3) are only available up to August 2017. WeRateDogs started in November 2015. What we accept are missing dog names and dog stages.\n",
    "\n",
    "Based on the Tweet IDs in (1) we collect the data from the API. We do not filter the IDs we request over the API based on the date given in (1). Instead, we filter the date directly at the source: for each Tweet ID data is collected using the Twitter API. The data returned is only accepted when the tweet is from before 02.08.2017. Also, retweets are filtered here.\n",
    "\n",
    "The Twitter API runs about 30 minutes to check 2,356 IDs in (1). Of those IDs, 2,325 are considered relevant. Additionally to the acceptance criteria mentioned, a small number of Tweets were deleted from Twitter as of 12.05.2019. Unexpectedly, the Twitter output contains a small 2-digit number of duplicates. One instance of the duplicates is dropped. Because of the runtime of data collection using the API, the data is stored on disc.\n",
    "\n",
    "Data completeness and tidiness is reached combining all three datasets, dropping columns which are duplicated or not relevant for the analysis, and melting the dog stages. From this, we end up with 2,065 observations. That is the number of lines in (3) minus Tweets no more available.\n",
    "\n",
    "Assessing data quality, we decide to correct the data type for the Tweet creation timestamp and improve the rating information. The rating information is wrong e.g. when similar patterns are in the Tweet. An example is \"3 1/2 legged (...) 9/10\", where 9/10 is the rating. Rating denominator is not 10 when multiple dogs on a single picture are rated. Instead of scaling, due to the small number of such pictures those observations are dropped. After this, we are down to 2,045 observations.\n",
    "\n",
    "The dogs' names are wrongly extracted in (1) for certain cases. Then wrongly \"a\", \"an\", and \"the\" are given as names. We correct these cases.\n",
    "\n",
    "The cleaned dataset is stored in CSV format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This report has 491 words.\n"
     ]
    }
   ],
   "source": [
    "# words:\n",
    "text = \"WeRateDogs is a Twitter account that rates people's dogs with a humorous comment about the dog. We wrangle the WeRateDogs Tweets. We provide a cleaned dataset for further analysis. We combine three datasets: 1. WeRateDogs Twitter archive download by WeRateDogs. 2. Data downloaded using the Twitter API. 3. Predictions of the pictures content. CMuch Tweet information can be obtained through both (1) and (2). Because the ETL steps obtaining the data directly from the source are more transparent, we prefer to collect the data from the Twitter API. The API is called Tweepy. Our approach is to collect all the data which is available via the API. We select the relevant columns later. Thus we can easily expand the analysis to data not considered before. Our goal is to produce a high quality dataset. We prefer a complete dataset over collecting as much data as possible. This means we combine the information from all sources and drop observations with missing data. Most notably, the predictions in (3) are only available up to August 2017. WeRateDogs was started November 2015. What we accept is missing dog names and dog stages.Based on the Tweet IDs in (1) we collect the data from the API. We do not filter the IDs we request over the API based on the date given in (1). Instead, we filter the date directly at the source: for each Tweet ID data is collected using the Twitter API. The data returned is only accepted when the tweet is from before 02.08.2017. Also retweets are filtered here.The Twitter API runs about 30 minutes to check 2,356 IDs in (1). Of those IDs 2,325 are considered relevant. Additionally to the acceptance criteria mentioned, a small amount of Tweets were deleted from Twitter as of 12.05.2019. Unexpectedly, the Twitter output contains a small 2-digit number of duplicates. One instance of the duplicates is dropped. Because of the runtime of data collection using the API, the data is stored on disc.Data completeness and tidiness is reached combining all three datasets, dropping columns which are duplicated or not relevant for the analysis, and melting the dog stages. From this we end up with 2,065 observations. That is the number of lines in (3) minus Tweets no more available. Assessing data quality, we decide to correct the data type for the Tweet creation timestamp and improve the rating information. The rating information is wrong e.g. when similar patterns are in the Tweet. An example is '3 1/2 legged (...) 9/10', where 9/10 is the rating. Ratings denominator is not 10 when multiple dogs on a single picture are rated. Instead of scaling, due to the small amount of such pictures those observations are dropped. After this we are down to 2,045 observations. The dogs names are wrongly extracted in (1) for certain cases. Then wrongly 'a', 'an', and 'the' are given as names. We correct these ceses. The cleaned dataset is stored in csv format.\"\n",
    "print('This report has', len(text.split(' ')), 'words.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
